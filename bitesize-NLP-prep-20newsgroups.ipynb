{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preparing text for Natural Language Processing\n",
    "Text wrangling is 80% of your battle when trying to topic model, phrase extract, or otherwise throw computer code at words. Today we'll be trying some standard techniques on sci-kit learn's 20newsgroups data to see how it turns out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.wordnet import WordNetLemmatizer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "stop_words = stopwords.words(\"english\") #load the stop words (words to ignore list) for english\n",
    "df = pd.DataFrame(pd.Series(fetch_20newsgroups(subset='train').data))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example entry from 20newsgroups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"From: lerxst@wam.umd.edu (where's my thing)\\nSubject: WHAT car is this!?\\nNntp-Posting-Host: rac3.wam.umd.edu\\nOrganization: University of Maryland, College Park\\nLines: 15\\n\\n I was wondering if anyone out there could enlighten me on this car I saw\\nthe other day. It was a 2-door sports car, looked to be from the late 60s/\\nearly 70s. It was called a Bricklin. The doors were really small. In addition,\\nthe front bumper was separate from the rest of the body. This is \\nall I know. If anyone can tellme a model name, engine specs, years\\nof production, where this car is made, history, or whatever info you\\nhave on this funky looking car, please e-mail.\\n\\nThanks,\\n- IL\\n   ---- brought to you by your neighborhood Lerxst ----\\n\\n\\n\\n\\n\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "From: lerxst@wam.umd.edu (where's my thing)\n",
    "Subject: WHAT car is this!?\n",
    "Nntp-Posting-Host: rac3.wam.umd.edu\n",
    "Organization: University of Maryland, College Park\n",
    "Lines: 15\n",
    "\n",
    " I was wondering if anyone out there could enlighten me on this car I saw\n",
    "the other day. It was a 2-door sports car, looked to be from the late 60s/\n",
    "early 70s. It was called a Bricklin. The doors were really small. In addition,\n",
    "the front bumper was separate from the rest of the body. This is \n",
    "all I know. If anyone can tellme a model name, engine specs, years\n",
    "of production, where this car is made, history, or whatever info you\n",
    "have on this funky looking car, please e-mail.\n",
    "\n",
    "Thanks,\n",
    "- IL\n",
    "   ---- brought to you by your neighborhood Lerxst ----\n",
    "'''\n",
    "# Print how python sees the string, it may help determine where you could split the data\n",
    "s = df[0][0]\n",
    "s"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do seperate signal from noise?\n",
    "\n",
    "The first thing you should do is decide what is important to your task, and what isn't. After that, you can see if there are underlying structures you can use to eliminate swaths of your text at once. \n",
    "<br>\n",
    "In our case, we only care about the comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extractComments(x):\n",
    "    ''' INPUT: a string\n",
    "        OUTPUT: the right side of the string after splitting it\n",
    "            on the first double line break\n",
    "    '''\n",
    "    l = x.split('\\n\\n',1)\n",
    "    return l[1]\n",
    "\n",
    "df['comments'] = df[0].apply(lambda x: extractComments(x)).astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we've broken out the comments from everything else, we can strip out non-words, links, and email addresses using regex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrubString(x):\n",
    "    ''' INPUT: a string\n",
    "        OUTPUT: a string that has had links removed, then non-letters, then english stopwords\n",
    "            This will produce a blank string if it only consisted of links, numbers, etc\n",
    "    '''\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    \n",
    "    x = re.sub(\"\\S*@\\S*\\s?\",\"\",x) #Remove email addresses\n",
    "    x = re.sub(\"#\\S+|&\\S+|@\\S+|https?:\\S+|RT|[^A-Za-z0-9]+\",' ', x) #Remove hyperlinks\n",
    "    x = re.sub(\"&\\S*|@\\S+|https?:\\S+\",' ', x) #Remove more hyperlinks\n",
    "    x = re.sub(\"[^A-Za-z']+\",' ',x) #keep only letters\n",
    "\n",
    "    if len(x)==0:\n",
    "        return ''\n",
    "    \n",
    "    tokens = word_tokenize(x) # Convert the string into tokens\n",
    "    \n",
    "    # Lemmatize the words, and only keep non-stop words\n",
    "    tokens = [lemmatizer.lemmatize(word).strip() for word in tokens if word not in stop_words]\n",
    "    \n",
    "    if len(tokens)==0:\n",
    "        return ''\n",
    "    \n",
    "    return ' '.join(map(str,tokens))\n",
    "\n",
    "df['cleaned'] = df['comments'].apply(lambda x: scrubString(x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The text contained in the \"cleaned\" column is now ready for preliminary analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you want to save the cleaned text for use in another project, uncomment & run the following code\n",
    "# df['cleaned'].to_csv('cleanted_20newsgroups.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
